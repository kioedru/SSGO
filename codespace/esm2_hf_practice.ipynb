{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/Kioedru/code/SSGO\")\n",
    "from codespace.utils.read_finetune_data import read_labels\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = read_labels(\"train\", \"P\", \"9606\")\n",
    "valid_labels = read_labels(\"valid\", \"P\", \"9606\")\n",
    "test_labels = read_labels(\"test\", \"P\", \"9606\")\n",
    "combine_labels = np.concatenate((train_labels, valid_labels), axis=0)\n",
    "combine_labels = torch.from_numpy(combine_labels).float()\n",
    "test_labels = torch.from_numpy(test_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\n",
    "    \"/home/Kioedru/code/SSGO/data/finetune/9606/train_data_P.pkl\"\n",
    ").dropna(subset=[\"Sequence\"])\n",
    "valid_df = pd.read_pickle(\n",
    "    \"/home/Kioedru/code/SSGO/data/finetune/9606/valid_data_P.pkl\"\n",
    ").dropna(subset=[\"Sequence\"])\n",
    "test_df = pd.read_pickle(\n",
    "    \"/home/Kioedru/code/SSGO/data/finetune/9606/test_data_P.pkl\"\n",
    ").dropna(subset=[\"Sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3460, 45)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(180, 45)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "terms = pd.read_pickle(\"/home/Kioedru/code/SSGO/data/finetune/9606/terms_P.pkl\")\n",
    "mlb = MultiLabelBinarizer(classes=terms[\"terms\"].tolist())\n",
    "\n",
    "train_encode = np.array(mlb.fit_transform(train_df[\"annotations\"]))\n",
    "valid_encode = np.array(mlb.fit_transform(valid_df[\"annotations\"]))\n",
    "test_encode = np.array(mlb.fit_transform(test_df[\"annotations\"]))\n",
    "concat_encode = np.concatenate((train_encode, valid_encode))\n",
    "display(concat_encode.shape, test_encode.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_seq = train_df[\"Sequence\"].tolist()\n",
    "valid_seq = valid_df[\"Sequence\"].tolist()\n",
    "test_seq = test_df[\"Sequence\"].tolist()\n",
    "concat_seq = train_seq + valid_seq\n",
    "display(len(concat_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]),\n",
       " array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0]),\n",
       " array([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sequences, test_sequences, train_labels, test_labels = (\n",
    "    concat_seq,\n",
    "    test_seq,\n",
    "    concat_encode,\n",
    "    test_encode,\n",
    ")\n",
    "train_labels = [np.array(item) for item in train_labels]\n",
    "test_labels = [np.array(item) for item in test_labels]\n",
    "display(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_path = \"/home/Kioedru/code/SSGO/data/LLM/esm2\"\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(esm_path, max_length=300, truncation=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的序列已经被转换成了 input_ids，这是标记化后的序列，还有一个 attention_mask。注意力掩码处理了长度可变的序列的情况——在这些情况下，较短的序列会用空白的“填充”标记进行填充，而注意力掩码会用0填充，表示模型应该忽略这些标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 20, 6, 15, 10, 16, 21, 16, 15, 13, 15, 20, 19, 12, 11, 23, 5, 9, 19, 11, 21, 18, 19, 6, 6, 15, 15, 14, 13, 4, 14, 16, 11, 17, 18, 10, 10, 4, 14, 18, 13, 21, 23, 8, 4, 8, 4, 16, 14, 18, 7, 19, 14, 7, 23, 11, 14, 13, 6, 12, 7, 18, 13, 4, 4, 17, 12, 7, 14, 22, 4, 15, 15, 19, 6, 11, 17, 14, 8, 17, 6, 9, 15, 4, 13, 6, 10, 8, 4, 12, 15, 4, 17, 18, 8, 15, 17, 8, 9, 6, 15, 19, 21, 23, 14, 7, 4, 18, 11, 7, 18, 11, 17, 17, 11, 21, 12, 7, 5, 7, 10, 11, 11, 6, 17, 7, 19, 5, 19, 9, 5, 7, 9, 16, 4, 17, 12, 15, 5, 15, 17, 18, 10, 13, 4, 4, 11, 13, 9, 14, 18, 8, 10, 16, 13, 12, 12, 11, 4, 16, 13, 14, 11, 17, 4, 13, 15, 18, 17, 7, 8, 17, 18, 19, 21, 7, 15, 17, 17, 20, 15, 12, 12, 13, 14, 13, 9, 9, 15, 5, 15, 16, 13, 14, 8, 19, 19, 4, 15, 17, 11, 17, 5, 9, 11, 10, 9, 11, 4, 16, 9, 4, 19, 15, 9, 18, 15, 6, 13, 9, 12, 4, 5, 5, 11, 20, 15, 5, 14, 9, 15, 15, 15, 7, 13, 15, 4, 17, 5, 5, 21, 19, 8, 11, 6, 15, 7, 8, 5, 8, 18, 11, 8, 11, 5, 20, 7, 14, 9, 11, 11, 21, 9, 5, 5, 5, 12, 13, 9, 13, 7, 4, 10, 19, 16, 18, 7, 15, 15, 15, 6, 19, 7, 10, 4, 21, 11, 17, 15, 6, 13, 4, 17, 4, 9, 4, 21, 23, 13, 4, 11, 14, 15, 11, 23, 9, 17, 18, 12, 10, 4, 23, 15, 15, 21, 19, 19, 13, 6, 11, 12, 18, 21, 10, 8, 12, 10, 17, 18, 7, 12, 16, 6, 6, 13, 14, 11, 6, 11, 6, 11, 6, 6, 9, 8, 19, 22, 6, 15, 14, 18, 15, 13, 9, 18, 10, 14, 17, 4, 8, 21, 11, 6, 10, 6, 12, 4, 8, 20, 5, 17, 8, 6, 14, 17, 8, 17, 10, 8, 16, 18, 18, 12, 11, 18, 10, 8, 23, 5, 19, 4, 13, 15, 15, 21, 11, 12, 18, 6, 10, 7, 7, 6, 6, 18, 13, 7, 4, 11, 5, 20, 9, 17, 7, 9, 8, 13, 14, 15, 11, 13, 10, 14, 15, 9, 9, 12, 10, 12, 13, 5, 11, 11, 7, 18, 7, 13, 14, 19, 9, 9, 5, 13, 5, 16, 12, 5, 16, 9, 10, 15, 11, 16, 4, 15, 7, 5, 14, 9, 11, 15, 7, 15, 8, 8, 16, 14, 16, 5, 6, 8, 16, 6, 14, 16, 11, 18, 10, 16, 6, 7, 6, 15, 19, 12, 17, 14, 5, 5, 11, 15, 10, 5, 5, 9, 9, 9, 14, 8, 11, 8, 5, 11, 7, 14, 20, 8, 15, 15, 15, 14, 8, 10, 6, 18, 6, 13, 18, 8, 8, 22, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(train_tokenized[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [label.astype(np.float32) for label in train_labels]\n",
    "test_labels = [label.astype(np.float32) for label in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at /home/Kioedru/code/SSGO/data/LLM/esm2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EsmForSequenceClassification(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=240, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (classifier): EsmClassificationHead(\n",
      "    (dense): Linear(in_features=480, out_features=480, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=480, out_features=45, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EsmForSequenceClassification,\n",
    ")\n",
    "\n",
    "num_labels = 45  # Add 1 since 0 can be a label\n",
    "# model = EsmForSequenceClassification.from_pretrained(\n",
    "#     esm_path, num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    "# )\n",
    "model = EsmForSequenceClassification.from_pretrained(\n",
    "    esm_path,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ").to(\"cuda:1\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些警告告诉我们模型正在丢弃用于语言建模的一些权重（lm_head），并添加用于序列分类的一些权重（classifier）。这正是我们在想要在序列分类任务上微调语言模型时所期望的！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们初始化我们的 TrainingArguments。这些参数控制各种训练超参数，并将被传递给我们的 Trainer。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参\n",
    "batch_size = 1\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/home/Kioedru/code/SSGO/codespace/result\",\n",
    "    evaluation_strategy=\"epoch\",  # 指定评估策略为每个epoch结束后进行评估\n",
    "    save_strategy=\"epoch\",  # 设置为每个epoch结束后保存模型\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批次大小\n",
    "    per_device_eval_batch_size=batch_size,  # 每个设备上的评估批次大小\n",
    "    num_train_epochs=3,  # 训练的总epoch数量\n",
    "    weight_decay=0.01,  # 权重衰减（L2正则化）的强度，有助于防止过拟合\n",
    "    load_best_model_at_end=True,  # 在评估指标（这里是准确率）最佳时保存并加载该模型\n",
    "    metric_for_best_model=\"accuracy\",  # 选择最佳模型的评估指标\n",
    "    push_to_hub=False,  # 是否将模型推送到hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义用于评估模型的指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels != -100]\n",
    "    labels = labels[labels != -100]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,  # 此时传递tokenizer用于填充序列\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/Kioedru/code/SSGO\")\n",
    "from codespace.model import aslloss_adaptive\n",
    "\n",
    "loss = aslloss_adaptive.AsymmetricLossOptimized(\n",
    "    gamma_neg=int(2),\n",
    "    gamma_pos=int(0),\n",
    "    clip=float(0),\n",
    "    disable_torch_grad_focal_loss=1e-5,\n",
    "    eps=args.eps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Model, self).__init__()\n",
    "        self.pretrained = EsmModel.from_pretrained(\n",
    "            \"/home/Kioedru/code/SSGO/data/LLM/esm2\"\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(in_features=480, out_features=45)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = self.pretrained(\n",
    "                input_ids=input_ids, attention_mask=attention_mask\n",
    "            ).last_hidden_state\n",
    "\n",
    "        output = self.fc(last_hidden_state)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(output, labels)\n",
    "\n",
    "        return loss, output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaoyao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
