{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'codespace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodespace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultihead_attention_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_activation_fn\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmamba_ssm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mamba\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFC_Decoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'codespace'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from codespace.model.multihead_attention_transformer import _get_activation_fn\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "\n",
    "class FC_Decoder(nn.Module):\n",
    "    def __init__(self, num_class, dim_feedforward, dropout, input_num=3):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "\n",
    "        self.output_layer1 = nn.Linear(\n",
    "            dim_feedforward * input_num, dim_feedforward // input_num\n",
    "        )\n",
    "        self.activation1 = torch.nn.Sigmoid()\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.output_layer3 = nn.Linear(2 * (dim_feedforward // input_num), num_class)\n",
    "        self.residue_attn = Mamba(d_model=480, d_state=16, d_conv=4, expand=2)\n",
    "        self.residue_linear1 = nn.Linear(2000, dim_feedforward // input_num)\n",
    "        self.output_layer2 = nn.Linear(2 * (dim_feedforward // input_num), num_class)\n",
    "\n",
    "    def forward(self, hs, residue):  # hs[3, 32, 512] residue[32,2000,480]\n",
    "        residue = self.residue_attn(residue)  # residue[32,2000,480]\n",
    "        residue = nn.functional.adaptive_avg_pool1d(residue, output_size=1).squeeze(\n",
    "            -1\n",
    "        )  # residue[32,2000]\n",
    "        residue = self.residue_linear1(residue)  # residue[32,512//3]\n",
    "\n",
    "        # 维度转换 第0维和第1维互换\n",
    "        hs = hs.permute(1, 0, 2)  # [32, 3, 512]\n",
    "        # 按第一维度展开\n",
    "        hs = hs.flatten(1)  # [32,512*3]\n",
    "\n",
    "        hs = self.output_layer1(hs)  # [32,512//3]\n",
    "\n",
    "        conca_hs_residue = torch.cat((hs, residue), dim=1)\n",
    "        # sigmoid\n",
    "        conca_hs_residue = self.activation1(conca_hs_residue)\n",
    "        conca_hs_residue = self.dropout1(conca_hs_residue)\n",
    "        # (512//3,GO标签数)\n",
    "        out = self.output_layer3(conca_hs_residue)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hs = torch.rand(3, 32, 512).to(\"cuda:0\")\n",
    "    residue = torch.rand(32, 2000, 480).to(\"cuda:0\")\n",
    "    model = FC_Decoder(45, 512, 0.1).to(\"cuda:0\")\n",
    "    out = model(hs, residue)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_residue(usefor, aspect, model_name, organism_num):\n",
    "    residue_name = f\"{usefor}_residue_{aspect}.pkl\"\n",
    "    file_path = os.path.join(\n",
    "        finetune_data_path, organism_num, f\"residue_{model_name}\", residue_name\n",
    "    )\n",
    "\n",
    "    residue = pd.read_pickle(file_path)\n",
    "    # 找到张量的最小值和最大值\n",
    "    residue_min = residue.min(dim=2, keepdim=True).values\n",
    "    residue_max = residue.max(dim=2, keepdim=True).values\n",
    "\n",
    "    # 执行 min-max 归一化\n",
    "    residue = (residue - residue_min) / (residue_max - residue_min)\n",
    "    return residue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_in_kioedru = \"/home/kioedru/code/SSGO/data\"\n",
    "dataset_path_in_Kioedru = \"/home/Kioedru/code/SSGO/data\"\n",
    "\n",
    "if os.path.exists(dataset_path_in_kioedru):\n",
    "    dataset_path = dataset_path_in_kioedru\n",
    "else:\n",
    "    dataset_path = dataset_path_in_Kioedru\n",
    "\n",
    "finetune_data_path = os.path.join(dataset_path, \"finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "usefor = \"train\"\n",
    "aspect = \"P\"\n",
    "model_name = \"esm2\"\n",
    "organism_num = \"9606\"\n",
    "residue_name = f\"{usefor}_residue_{aspect}.pkl\"\n",
    "file_path = os.path.join(\n",
    "    finetune_data_path, organism_num, f\"residue_{model_name}\", residue_name\n",
    ")\n",
    "\n",
    "residue = pd.read_pickle(file_path)\n",
    "# # 找到张量的最小值和最大值\n",
    "# residue_min = residue.min(dim=2, keepdim=True).values\n",
    "# print(residue_min)\n",
    "# residue_max = residue.max(dim=2, keepdim=True).values\n",
    "\n",
    "# # 执行 min-max 归一化\n",
    "# residue = (residue - residue_min) / (residue_max - residue_min)\n",
    "layernorm = torch.nn.LayerNorm(480)\n",
    "residue = layernorm(residue)\n",
    "print(torch.isnan(residue).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
