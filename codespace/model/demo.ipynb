{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 45])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# from codespace.model.multihead_attention_transformer import _get_activation_fn\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "\n",
    "class FC_Decoder(nn.Module):\n",
    "    def __init__(self, num_class, dim_feedforward, dropout, input_num=3):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "\n",
    "        self.output_layer1 = nn.Linear(\n",
    "            dim_feedforward * input_num, dim_feedforward // input_num\n",
    "        )\n",
    "        self.activation1 = torch.nn.Sigmoid()\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.output_layer3 = nn.Linear(2 * (dim_feedforward // input_num), num_class)\n",
    "        self.residue_attn = Mamba(d_model=480, d_state=16, d_conv=4, expand=2)\n",
    "        self.residue_linear1 = nn.Linear(480, dim_feedforward // input_num)\n",
    "        self.output_layer2 = nn.Linear(2 * (dim_feedforward // input_num), num_class)\n",
    "\n",
    "    def forward(self, hs, residue):  # hs[3, 32, 512] residue[32,2000,480]\n",
    "        residue_score = self.residue_attn(residue)  # residue_score[32,2000,480]\n",
    "        residue_score = nn.functional.adaptive_avg_pool1d(\n",
    "            residue_score, output_size=1\n",
    "        )  # residue_score[32,2000,1]\n",
    "        residue_score = nn.functional.softmax(\n",
    "            residue_score, dim=1\n",
    "        )  # residue_score[32,2000,1]\n",
    "        residue = torch.sum(residue * residue_score, dim=1)  # [32,480]\n",
    "\n",
    "        residue = self.residue_linear1(residue)  # residue[32,512//3]\n",
    "\n",
    "        # 维度转换 第0维和第1维互换\n",
    "        hs = hs.permute(1, 0, 2)  # [32, 3, 512]\n",
    "        # 按第一维度展开\n",
    "        hs = hs.flatten(1)  # [32,512*3]\n",
    "\n",
    "        hs = self.output_layer1(hs)  # [32,512//3]\n",
    "\n",
    "        conca_hs_residue = torch.cat((hs, residue), dim=1)\n",
    "        # sigmoid\n",
    "        conca_hs_residue = self.activation1(conca_hs_residue)\n",
    "        conca_hs_residue = self.dropout1(conca_hs_residue)\n",
    "        # (512//3,GO标签数)\n",
    "        out = self.output_layer3(conca_hs_residue)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hs = torch.rand(3, 32, 512).to(\"cuda:0\")\n",
    "    residue = torch.rand(32, 2000, 480).to(\"cuda:0\")\n",
    "    model = FC_Decoder(45, 512, 0.1).to(\"cuda:0\")\n",
    "    out = model(hs, residue)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_residue(usefor, aspect, model_name, organism_num):\n",
    "    residue_name = f\"{usefor}_residue_{aspect}.pkl\"\n",
    "    file_path = os.path.join(\n",
    "        finetune_data_path, organism_num, f\"residue_{model_name}\", residue_name\n",
    "    )\n",
    "\n",
    "    residue = pd.read_pickle(file_path)\n",
    "    # 找到张量的最小值和最大值\n",
    "    residue_min = residue.min(dim=2, keepdim=True).values\n",
    "    residue_max = residue.max(dim=2, keepdim=True).values\n",
    "\n",
    "    # 执行 min-max 归一化\n",
    "    residue = (residue - residue_min) / (residue_max - residue_min)\n",
    "    return residue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_in_kioedru = \"/home/kioedru/code/SSGO/data\"\n",
    "dataset_path_in_Kioedru = \"/home/Kioedru/code/SSGO/data\"\n",
    "\n",
    "if os.path.exists(dataset_path_in_kioedru):\n",
    "    dataset_path = dataset_path_in_kioedru\n",
    "else:\n",
    "    dataset_path = dataset_path_in_Kioedru\n",
    "\n",
    "finetune_data_path = os.path.join(dataset_path, \"finetune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "usefor = \"train\"\n",
    "aspect = \"P\"\n",
    "model_name = \"esm2\"\n",
    "organism_num = \"9606\"\n",
    "residue_name = f\"{usefor}_residue_{aspect}.pkl\"\n",
    "file_path = os.path.join(\n",
    "    finetune_data_path, organism_num, f\"residue_{model_name}\", residue_name\n",
    ")\n",
    "\n",
    "residue = pd.read_pickle(file_path)\n",
    "# # 找到张量的最小值和最大值\n",
    "# residue_min = residue.min(dim=2, keepdim=True).values\n",
    "# print(residue_min)\n",
    "# residue_max = residue.max(dim=2, keepdim=True).values\n",
    "\n",
    "# # 执行 min-max 归一化\n",
    "# residue = (residue - residue_min) / (residue_max - residue_min)\n",
    "layernorm = torch.nn.LayerNorm(480)\n",
    "residue = layernorm(residue)\n",
    "print(torch.isnan(residue).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 45])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FC_Decoder(nn.Module):\n",
    "    def __init__(self, num_class, dim_feedforward, dropout, input_num=6):\n",
    "        super().__init__()\n",
    "        self.num_class = num_class\n",
    "\n",
    "        self.output_layer1 = nn.Linear(\n",
    "            dim_feedforward * input_num, dim_feedforward * int(input_num / 2)\n",
    "        )\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.output_layer2 = nn.Linear(\n",
    "            dim_feedforward * int(input_num / 2),\n",
    "            dim_feedforward // int((input_num / 2)),\n",
    "        )\n",
    "        self.activation2 = torch.nn.Sigmoid()\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.output_layer3 = nn.Linear(dim_feedforward // int(input_num / 2), num_class)\n",
    "\n",
    "        self.hs_transformer_linear = nn.Linear(3, 1)\n",
    "        self.hs_mamba_linear = nn.Linear(3, 1)\n",
    "        self.hs_P_linear = nn.Linear(2, 1)\n",
    "        self.hs_F_linear = nn.Linear(2, 1)\n",
    "        self.hs_C_linear = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, hs):  # hs[6, 32, 512] 前三个是transformer ， 后三个是mamba\n",
    "        # 维度转换 第0维和第1维互换\n",
    "        hs = hs.permute(1, 0, 2)  # [32, 6, 512]\n",
    "        avg_hs = nn.functional.adaptive_avg_pool1d(hs, output_size=1)  # avg_hs[32,6,1]\n",
    "        # 分成前面一半 hs_transformer [32, 3]\n",
    "        hs_transformer = avg_hs[:, :3, 0]\n",
    "        # 分成后面一半 hs_mamba [32, 3]\n",
    "        hs_mamba = avg_hs[:, 3:, 0]\n",
    "        hs_transformer = self.hs_transformer_linear(hs_transformer)  # [32,1]\n",
    "        hs_mamba = self.hs_mamba_linear(hs_mamba)  # [32,1]\n",
    "        hs_encoder = torch.cat((hs_transformer, hs_mamba), dim=1)  # [32,2]\n",
    "        hs_encoder = nn.functional.softmax(hs_encoder, dim=1)  # [32,2]\n",
    "\n",
    "        # 分别提取所需的列并移除第三个维度\n",
    "        hs_P = torch.cat(\n",
    "            (avg_hs[:, 0, 0].unsqueeze(1), avg_hs[:, 3, 0].unsqueeze(1)), dim=1\n",
    "        )\n",
    "        hs_F = torch.cat(\n",
    "            (avg_hs[:, 1, 0].unsqueeze(1), avg_hs[:, 4, 0].unsqueeze(1)), dim=1\n",
    "        )\n",
    "        hs_C = torch.cat(\n",
    "            (avg_hs[:, 2, 0].unsqueeze(1), avg_hs[:, 5, 0].unsqueeze(1)), dim=1\n",
    "        )\n",
    "        hs_P = self.hs_P_linear(hs_P)  # [32,1]\n",
    "        hs_F = self.hs_F_linear(hs_F)  # [32,1]\n",
    "        hs_C = self.hs_C_linear(hs_C)  # [32,1]\n",
    "        hs_aspect = torch.cat((hs_P, hs_F, hs_C), dim=1)  # [32,3]\n",
    "        hs_aspect = nn.functional.softmax(hs_aspect, dim=1)  # [32,3]\n",
    "\n",
    "        hs_transformer = hs_encoder[:, 0].unsqueeze(1)  # [32,1]\n",
    "        hs_mamba = hs_encoder[:, 1].unsqueeze(1)  # [32,1]\n",
    "\n",
    "        hs_transformer_aspect = hs_transformer * hs_aspect  # [32,3]\n",
    "        hs_mamba_aspect = hs_mamba * hs_aspect  # [32,3]\n",
    "        sig_hs = torch.cat((hs_transformer_aspect, hs_mamba_aspect), dim=1)  # [32, 6]\n",
    "        sig_hs = sig_hs.unsqueeze(2)  # [32,6,1]\n",
    "        hs = sig_hs * hs  # [32,6,512]\n",
    "\n",
    "        # 按第一维度展开\n",
    "        hs = hs.flatten(1)  # [32,512*6]\n",
    "        # 默认(512*2,512//2)，//表示下取整\n",
    "        hs = self.output_layer1(hs)  # [32,512*(6/2)]\n",
    "        hs = self.activation1(hs)\n",
    "        hs = self.dropout1(hs)\n",
    "\n",
    "        hs = self.output_layer2(hs)  # [32,512//(6/2)]\n",
    "        hs = self.activation2(hs)\n",
    "        hs = self.dropout2(hs)\n",
    "\n",
    "        # (512//2,GO标签数)\n",
    "        out = self.output_layer3(hs)\n",
    "        # 后面还需要一个sigmoid，在测试输出后面直接加了\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hs = torch.rand(6, 32, 512).to(\"cuda:0\")\n",
    "    model = FC_Decoder(45, 512, 0.1).to(\"cuda:0\")\n",
    "    out = model(hs)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result shape: torch.Size([32, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设已经有形状为 [32, 2] 的张量 a 和形状为 [32, 3] 的张量 b\n",
    "a = torch.randn(32, 2)\n",
    "b = torch.randn(32, 3)\n",
    "\n",
    "# 分别提取 a 的第0列和第1列\n",
    "a0 = a[:, 0].unsqueeze(1)  # 形状 [32, 1]\n",
    "a1 = a[:, 1].unsqueeze(1)  # 形状 [32, 1]\n",
    "\n",
    "# 计算 a 的第0列与 b 的点乘\n",
    "result0 = a0 * b  # 形状 [32, 3]\n",
    "\n",
    "# 计算 a 的第1列与 b 的点乘\n",
    "result1 = a1 * b  # 形状 [32, 3]\n",
    "\n",
    "# 拼接结果\n",
    "result = torch.cat((result0, result1), dim=1)  # 形状 [32, 6]\n",
    "\n",
    "# 打印结果以检查形状\n",
    "print(\"result shape:\", result.shape)  # 应该是 [32, 6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
