{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "# 定义预训练模型\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation,\n",
    "        dim_feedforward=2048,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        encoder_layer = Encoder_Layer(\n",
    "            dim_feedforward,  # 512\n",
    "            nhead,  # 8\n",
    "            dropout,  # 0.1\n",
    "            activation,  # gelu\n",
    "        )\n",
    "        # 设置多层encoder_layer\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [copy.deepcopy(encoder_layer) for _ in range(num_encoder_layers)]\n",
    "        )\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, src):\n",
    "        out = src\n",
    "        for module in self.encoder:\n",
    "            out = module(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义encoder_layer\n",
    "class Encoder_Layer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_feedforward,\n",
    "        nhead,  # 8\n",
    "        dropout,\n",
    "        activation,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_attn = nn.MultiheadAttention(dim_feedforward, nhead, dropout=dropout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim_feedforward, 2048),\n",
    "            activation,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2048, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim_feedforward)\n",
    "        self.norm2 = nn.LayerNorm(dim_feedforward)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,  # 3,32,512\n",
    "    ):\n",
    "\n",
    "        # 自注意力\n",
    "        src2, corr = self.self_attn(\n",
    "            query=src,\n",
    "            key=src,\n",
    "            value=src,\n",
    "        )\n",
    "\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.fc(src)\n",
    "        src = src + src2\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "model = TransformerEncoder(\n",
    "    nn.GELU(), dim_feedforward=512, nhead=8, num_encoder_layers=6, dropout=0.1\n",
    ")\n",
    "src = torch.rand(3, 32, 512)\n",
    "output = model(src)\n",
    "print(output.shape)\n",
    "torch.save(model.state_dict(), \"demo_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finetune(nn.Module):\n",
    "    def __init__(self, dim_feedforward, nhead, dropout, class_num, num_encoder_layers):\n",
    "        super().__init__()\n",
    "        # 载入预训练模型\n",
    "        pretrain_model = TransformerEncoder(\n",
    "            nn.GELU(),\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        pretrain_model.load_state_dict(torch.load(\"demo_model.pth\"))\n",
    "        self.pretrain = pretrain_model\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim_feedforward * 3, dim_feedforward),\n",
    "            nn.Linear(dim_feedforward, class_num),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input,  # 3,32,512\n",
    "    ):\n",
    "\n",
    "        output = self.pretrain(input)\n",
    "        output = torch.einsum(\"LBD->BLD\", output).flatten(1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "finetune_model = Finetune(512, 8, 0.1, 10, 6)\n",
    "input = torch.rand(3, 32, 512)\n",
    "output = finetune_model(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation,\n",
    "        dim_feedforward=2048,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        encoder_layer = Encoder_Layer(\n",
    "            dim_feedforward,  # 512\n",
    "            nhead,  # 8\n",
    "            dropout,  # 0.1\n",
    "            activation,  # gelu\n",
    "        )\n",
    "        # 设置多层encoder_layer\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [copy.deepcopy(encoder_layer) for _ in range(num_encoder_layers)]\n",
    "        )\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, src):\n",
    "        out = src\n",
    "        for module in self.encoder:\n",
    "            out = module(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Finetune(TransformerEncoder):\n",
    "    def __init__(\n",
    "        self, activation, dim_feedforward, nhead, num_encoder_layers, dropout, class_num\n",
    "    ):\n",
    "        super().__init__(\n",
    "            activation, dim_feedforward, nhead, num_encoder_layers, dropout\n",
    "        )\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        # 载入预训练模型\n",
    "        cross_attention = nn.MultiheadAttention(dim_feedforward, nhead, dropout=dropout)\n",
    "        self.cross_layer1 = nn.ModuleList(\n",
    "            [copy.deepcopy(cross_attention) for _ in range(num_encoder_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output = input1\n",
    "        for num in self.num_encoder_layers:\n",
    "            output = self.cross_layer[num](output, input2, input2)\n",
    "            output = self.encoder[num](output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune_model = Finetune(nn.GELU(), 512, 8, 6, 0.1, 10)\n",
    "pretrained_state_dict = torch.load(\"demo_model.pth\")\n",
    "model_state_dict = finetune_model.state_dict()\n",
    "\n",
    "# 过滤掉不匹配的键\n",
    "pretrained_state_dict = {\n",
    "    k: v for k, v in pretrained_state_dict.items() if k in model_state_dict\n",
    "}\n",
    "\n",
    "# 更新现有的state_dict\n",
    "model_state_dict.update(pretrained_state_dict)\n",
    "\n",
    "# 加载更新后的state_dict\n",
    "finetune_model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.self_attn.in_proj_weight: No\n",
      "encoder.0.self_attn.in_proj_bias: No\n",
      "encoder.0.self_attn.out_proj.weight: No\n",
      "encoder.0.self_attn.out_proj.bias: No\n",
      "encoder.0.fc.0.weight: No\n",
      "encoder.0.fc.0.bias: No\n",
      "encoder.0.fc.3.weight: No\n",
      "encoder.0.fc.3.bias: No\n",
      "encoder.0.norm1.weight: No\n",
      "encoder.0.norm1.bias: No\n",
      "encoder.0.norm2.weight: No\n",
      "encoder.0.norm2.bias: No\n",
      "encoder.1.self_attn.in_proj_weight: No\n",
      "encoder.1.self_attn.in_proj_bias: No\n",
      "encoder.1.self_attn.out_proj.weight: No\n",
      "encoder.1.self_attn.out_proj.bias: No\n",
      "encoder.1.fc.0.weight: No\n",
      "encoder.1.fc.0.bias: No\n",
      "encoder.1.fc.3.weight: No\n",
      "encoder.1.fc.3.bias: No\n",
      "encoder.1.norm1.weight: No\n",
      "encoder.1.norm1.bias: No\n",
      "encoder.1.norm2.weight: No\n",
      "encoder.1.norm2.bias: No\n",
      "encoder.2.self_attn.in_proj_weight: No\n",
      "encoder.2.self_attn.in_proj_bias: No\n",
      "encoder.2.self_attn.out_proj.weight: No\n",
      "encoder.2.self_attn.out_proj.bias: No\n",
      "encoder.2.fc.0.weight: No\n",
      "encoder.2.fc.0.bias: No\n",
      "encoder.2.fc.3.weight: No\n",
      "encoder.2.fc.3.bias: No\n",
      "encoder.2.norm1.weight: No\n",
      "encoder.2.norm1.bias: No\n",
      "encoder.2.norm2.weight: No\n",
      "encoder.2.norm2.bias: No\n",
      "encoder.3.self_attn.in_proj_weight: No\n",
      "encoder.3.self_attn.in_proj_bias: No\n",
      "encoder.3.self_attn.out_proj.weight: No\n",
      "encoder.3.self_attn.out_proj.bias: No\n",
      "encoder.3.fc.0.weight: No\n",
      "encoder.3.fc.0.bias: No\n",
      "encoder.3.fc.3.weight: No\n",
      "encoder.3.fc.3.bias: No\n",
      "encoder.3.norm1.weight: No\n",
      "encoder.3.norm1.bias: No\n",
      "encoder.3.norm2.weight: No\n",
      "encoder.3.norm2.bias: No\n",
      "encoder.4.self_attn.in_proj_weight: No\n",
      "encoder.4.self_attn.in_proj_bias: No\n",
      "encoder.4.self_attn.out_proj.weight: No\n",
      "encoder.4.self_attn.out_proj.bias: No\n",
      "encoder.4.fc.0.weight: No\n",
      "encoder.4.fc.0.bias: No\n",
      "encoder.4.fc.3.weight: No\n",
      "encoder.4.fc.3.bias: No\n",
      "encoder.4.norm1.weight: No\n",
      "encoder.4.norm1.bias: No\n",
      "encoder.4.norm2.weight: No\n",
      "encoder.4.norm2.bias: No\n",
      "encoder.5.self_attn.in_proj_weight: No\n",
      "encoder.5.self_attn.in_proj_bias: No\n",
      "encoder.5.self_attn.out_proj.weight: No\n",
      "encoder.5.self_attn.out_proj.bias: No\n",
      "encoder.5.fc.0.weight: No\n",
      "encoder.5.fc.0.bias: No\n",
      "encoder.5.fc.3.weight: No\n",
      "encoder.5.fc.3.bias: No\n",
      "encoder.5.norm1.weight: No\n",
      "encoder.5.norm1.bias: No\n",
      "encoder.5.norm2.weight: No\n",
      "encoder.5.norm2.bias: No\n",
      "cross_layer1.0.in_proj_weight: Yes\n",
      "cross_layer1.0.in_proj_bias: Yes\n",
      "cross_layer1.0.out_proj.weight: Yes\n",
      "cross_layer1.0.out_proj.bias: Yes\n",
      "cross_layer1.1.in_proj_weight: Yes\n",
      "cross_layer1.1.in_proj_bias: Yes\n",
      "cross_layer1.1.out_proj.weight: Yes\n",
      "cross_layer1.1.out_proj.bias: Yes\n",
      "cross_layer1.2.in_proj_weight: Yes\n",
      "cross_layer1.2.in_proj_bias: Yes\n",
      "cross_layer1.2.out_proj.weight: Yes\n",
      "cross_layer1.2.out_proj.bias: Yes\n",
      "cross_layer1.3.in_proj_weight: Yes\n",
      "cross_layer1.3.in_proj_bias: Yes\n",
      "cross_layer1.3.out_proj.weight: Yes\n",
      "cross_layer1.3.out_proj.bias: Yes\n",
      "cross_layer1.4.in_proj_weight: Yes\n",
      "cross_layer1.4.in_proj_bias: Yes\n",
      "cross_layer1.4.out_proj.weight: Yes\n",
      "cross_layer1.4.out_proj.bias: Yes\n",
      "cross_layer1.5.in_proj_weight: Yes\n",
      "cross_layer1.5.in_proj_bias: Yes\n",
      "cross_layer1.5.out_proj.weight: Yes\n",
      "cross_layer1.5.out_proj.bias: Yes\n"
     ]
    }
   ],
   "source": [
    "# 假设 finetune_model 是已经加载了预训练权重的模型\n",
    "\n",
    "# 冻结原模型（TransformerEncoder）的参数\n",
    "for param in finetune_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 只训练新添加的参数\n",
    "# 例如 cross_layer1 和其他可能的新参数\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=1e-4\n",
    ")\n",
    "\n",
    "# 检查哪些参数会被训练\n",
    "for name, param in finetune_model.named_parameters():\n",
    "    print(f\"{name}: {'Yes' if param.requires_grad else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal:\n",
    "    def __init__(self, name):\n",
    "        pass\n",
    "\n",
    "    def speak(self):\n",
    "        return \"Some sound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cat(Animal):\n",
    "    def __init__(self, name, color):\n",
    "        super().__init__(name)  # 调用父类的构造方法\n",
    "        self.color = color  # 新属性\n",
    "\n",
    "    def speak(self):\n",
    "        return \"Meow!\"\n",
    "\n",
    "    def purr(self):  # 新方法\n",
    "        return \"Purr...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xianluo = Cat(\"xianluo\", \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 假设这是原始模型类\n",
    "class OriginalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OriginalModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.layer2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 新模型类，继承自原始模型\n",
    "class ModifiedModel(OriginalModel):\n",
    "    def __init__(self):\n",
    "        super(ModifiedModel, self).__init__()\n",
    "\n",
    "    # 重写forward函数\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        # 在这里加入新的逻辑\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 加载原模型的权重\n",
    "original_model = OriginalModel()\n",
    "# 假设 original_model 已经训练好并保存\n",
    "torch.save(original_model.state_dict(), \"model.pth\")\n",
    "\n",
    "# 创建新模型实例\n",
    "modified_model = ModifiedModel()\n",
    "modified_model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# 使用修改后的模型\n",
    "input_tensor = torch.randn(1, 10)\n",
    "output = modified_model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0-5): 6 x TransformerEncoderLayer(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (self_attn): BiMamba(\n",
      "      (in_proj): Linear(in_features=1, out_features=4, bias=False)\n",
      "      (conv1d): Conv1d(2, 2, kernel_size=(4,), stride=(1,), padding=(3,), groups=2)\n",
      "      (conv1d_b): Conv1d(2, 2, kernel_size=(4,), stride=(1,), padding=(3,), groups=2)\n",
      "      (act): SiLU()\n",
      "      (x_proj): Linear(in_features=2, out_features=33, bias=False)\n",
      "      (x_proj_b): Linear(in_features=2, out_features=33, bias=False)\n",
      "      (dt_proj): Linear(in_features=1, out_features=2, bias=True)\n",
      "      (dt_proj_b): Linear(in_features=1, out_features=2, bias=True)\n",
      "      (out_proj): Linear(in_features=2, out_features=1, bias=False)\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "transformerEncoder.encoder.layers.0.self_attn.A_log: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.A_b_log: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.D: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.D_b: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.in_proj.weight: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.conv1d.weight: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.conv1d.bias: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.conv1d_b.weight: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.conv1d_b.bias: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.x_proj.weight: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.x_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.dt_proj.weight: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.dt_proj.bias: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.dt_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.dt_proj_b.bias: No\n",
      "transformerEncoder.encoder.layers.0.self_attn.out_proj.weight: No\n",
      "transformerEncoder.encoder.layers.0.linear1.weight: Yes\n",
      "transformerEncoder.encoder.layers.0.linear1.bias: Yes\n",
      "transformerEncoder.encoder.layers.0.linear2.weight: Yes\n",
      "transformerEncoder.encoder.layers.0.linear2.bias: Yes\n",
      "transformerEncoder.encoder.layers.0.norm1.weight: Yes\n",
      "transformerEncoder.encoder.layers.0.norm1.bias: Yes\n",
      "transformerEncoder.encoder.layers.0.norm2.weight: Yes\n",
      "transformerEncoder.encoder.layers.0.norm2.bias: Yes\n",
      "transformerEncoder.encoder.layers.1.self_attn.A_log: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.A_b_log: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.D: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.D_b: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.in_proj.weight: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.conv1d.weight: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.conv1d.bias: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.conv1d_b.weight: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.conv1d_b.bias: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.x_proj.weight: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.x_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.dt_proj.weight: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.dt_proj.bias: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.dt_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.dt_proj_b.bias: No\n",
      "transformerEncoder.encoder.layers.1.self_attn.out_proj.weight: No\n",
      "transformerEncoder.encoder.layers.1.linear1.weight: Yes\n",
      "transformerEncoder.encoder.layers.1.linear1.bias: Yes\n",
      "transformerEncoder.encoder.layers.1.linear2.weight: Yes\n",
      "transformerEncoder.encoder.layers.1.linear2.bias: Yes\n",
      "transformerEncoder.encoder.layers.1.norm1.weight: Yes\n",
      "transformerEncoder.encoder.layers.1.norm1.bias: Yes\n",
      "transformerEncoder.encoder.layers.1.norm2.weight: Yes\n",
      "transformerEncoder.encoder.layers.1.norm2.bias: Yes\n",
      "transformerEncoder.encoder.layers.2.self_attn.A_log: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.A_b_log: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.D: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.D_b: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.in_proj.weight: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.conv1d.weight: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.conv1d.bias: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.conv1d_b.weight: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.conv1d_b.bias: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.x_proj.weight: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.x_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.dt_proj.weight: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.dt_proj.bias: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.dt_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.dt_proj_b.bias: No\n",
      "transformerEncoder.encoder.layers.2.self_attn.out_proj.weight: No\n",
      "transformerEncoder.encoder.layers.2.linear1.weight: Yes\n",
      "transformerEncoder.encoder.layers.2.linear1.bias: Yes\n",
      "transformerEncoder.encoder.layers.2.linear2.weight: Yes\n",
      "transformerEncoder.encoder.layers.2.linear2.bias: Yes\n",
      "transformerEncoder.encoder.layers.2.norm1.weight: Yes\n",
      "transformerEncoder.encoder.layers.2.norm1.bias: Yes\n",
      "transformerEncoder.encoder.layers.2.norm2.weight: Yes\n",
      "transformerEncoder.encoder.layers.2.norm2.bias: Yes\n",
      "transformerEncoder.encoder.layers.3.self_attn.A_log: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.A_b_log: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.D: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.D_b: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.in_proj.weight: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.conv1d.weight: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.conv1d.bias: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.conv1d_b.weight: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.conv1d_b.bias: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.x_proj.weight: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.x_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.dt_proj.weight: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.dt_proj.bias: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.dt_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.dt_proj_b.bias: No\n",
      "transformerEncoder.encoder.layers.3.self_attn.out_proj.weight: No\n",
      "transformerEncoder.encoder.layers.3.linear1.weight: Yes\n",
      "transformerEncoder.encoder.layers.3.linear1.bias: Yes\n",
      "transformerEncoder.encoder.layers.3.linear2.weight: Yes\n",
      "transformerEncoder.encoder.layers.3.linear2.bias: Yes\n",
      "transformerEncoder.encoder.layers.3.norm1.weight: Yes\n",
      "transformerEncoder.encoder.layers.3.norm1.bias: Yes\n",
      "transformerEncoder.encoder.layers.3.norm2.weight: Yes\n",
      "transformerEncoder.encoder.layers.3.norm2.bias: Yes\n",
      "transformerEncoder.encoder.layers.4.self_attn.A_log: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.A_b_log: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.D: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.D_b: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.in_proj.weight: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.conv1d.weight: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.conv1d.bias: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.conv1d_b.weight: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.conv1d_b.bias: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.x_proj.weight: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.x_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.dt_proj.weight: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.dt_proj.bias: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.dt_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.dt_proj_b.bias: No\n",
      "transformerEncoder.encoder.layers.4.self_attn.out_proj.weight: No\n",
      "transformerEncoder.encoder.layers.4.linear1.weight: Yes\n",
      "transformerEncoder.encoder.layers.4.linear1.bias: Yes\n",
      "transformerEncoder.encoder.layers.4.linear2.weight: Yes\n",
      "transformerEncoder.encoder.layers.4.linear2.bias: Yes\n",
      "transformerEncoder.encoder.layers.4.norm1.weight: Yes\n",
      "transformerEncoder.encoder.layers.4.norm1.bias: Yes\n",
      "transformerEncoder.encoder.layers.4.norm2.weight: Yes\n",
      "transformerEncoder.encoder.layers.4.norm2.bias: Yes\n",
      "transformerEncoder.encoder.layers.5.self_attn.A_log: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.A_b_log: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.D: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.D_b: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.in_proj.weight: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.conv1d.weight: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.conv1d.bias: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.conv1d_b.weight: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.conv1d_b.bias: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.x_proj.weight: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.x_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.dt_proj.weight: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.dt_proj.bias: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.dt_proj_b.weight: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.dt_proj_b.bias: No\n",
      "transformerEncoder.encoder.layers.5.self_attn.out_proj.weight: No\n",
      "transformerEncoder.encoder.layers.5.linear1.weight: Yes\n",
      "transformerEncoder.encoder.layers.5.linear1.bias: Yes\n",
      "transformerEncoder.encoder.layers.5.linear2.weight: Yes\n",
      "transformerEncoder.encoder.layers.5.linear2.bias: Yes\n",
      "transformerEncoder.encoder.layers.5.norm1.weight: Yes\n",
      "transformerEncoder.encoder.layers.5.norm1.bias: Yes\n",
      "transformerEncoder.encoder.layers.5.norm2.weight: Yes\n",
      "transformerEncoder.encoder.layers.5.norm2.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.A_log: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.A_b_log: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.D: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.D_b: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.in_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.conv1d.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.conv1d.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.conv1d_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.conv1d_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.x_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.x_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.dt_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.dt_proj.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.dt_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.dt_proj_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.self_attn.out_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.linear1.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.linear1.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.linear2.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.linear2.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.norm1.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.norm1.bias: Yes\n",
      "transformerDecoder.encoder.layers.0.norm2.weight: Yes\n",
      "transformerDecoder.encoder.layers.0.norm2.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.A_log: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.A_b_log: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.D: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.D_b: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.in_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.conv1d.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.conv1d.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.conv1d_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.conv1d_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.x_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.x_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.dt_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.dt_proj.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.dt_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.dt_proj_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.self_attn.out_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.linear1.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.linear1.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.linear2.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.linear2.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.norm1.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.norm1.bias: Yes\n",
      "transformerDecoder.encoder.layers.1.norm2.weight: Yes\n",
      "transformerDecoder.encoder.layers.1.norm2.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.A_log: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.A_b_log: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.D: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.D_b: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.in_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.conv1d.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.conv1d.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.conv1d_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.conv1d_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.x_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.x_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.dt_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.dt_proj.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.dt_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.dt_proj_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.self_attn.out_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.linear1.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.linear1.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.linear2.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.linear2.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.norm1.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.norm1.bias: Yes\n",
      "transformerDecoder.encoder.layers.2.norm2.weight: Yes\n",
      "transformerDecoder.encoder.layers.2.norm2.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.A_log: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.A_b_log: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.D: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.D_b: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.in_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.conv1d.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.conv1d.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.conv1d_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.conv1d_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.x_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.x_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.dt_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.dt_proj.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.dt_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.dt_proj_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.self_attn.out_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.linear1.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.linear1.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.linear2.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.linear2.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.norm1.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.norm1.bias: Yes\n",
      "transformerDecoder.encoder.layers.3.norm2.weight: Yes\n",
      "transformerDecoder.encoder.layers.3.norm2.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.A_log: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.A_b_log: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.D: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.D_b: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.in_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.conv1d.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.conv1d.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.conv1d_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.conv1d_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.x_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.x_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.dt_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.dt_proj.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.dt_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.dt_proj_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.self_attn.out_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.linear1.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.linear1.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.linear2.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.linear2.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.norm1.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.norm1.bias: Yes\n",
      "transformerDecoder.encoder.layers.4.norm2.weight: Yes\n",
      "transformerDecoder.encoder.layers.4.norm2.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.A_log: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.A_b_log: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.D: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.D_b: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.in_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.conv1d.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.conv1d.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.conv1d_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.conv1d_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.x_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.x_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.dt_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.dt_proj.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.dt_proj_b.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.dt_proj_b.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.self_attn.out_proj.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.linear1.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.linear1.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.linear2.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.linear2.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.norm1.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.norm1.bias: Yes\n",
      "transformerDecoder.encoder.layers.5.norm2.weight: Yes\n",
      "transformerDecoder.encoder.layers.5.norm2.bias: Yes\n",
      "input_proj_x1.weight: Yes\n",
      "input_proj_x1.bias: Yes\n",
      "input_proj_z1.weight: Yes\n",
      "input_proj_z1.bias: Yes\n",
      "input_proj_s1.weight: Yes\n",
      "input_proj_s1.bias: Yes\n",
      "input_proj_x2.weight: Yes\n",
      "input_proj_x2.bias: Yes\n",
      "input_proj_z2.weight: Yes\n",
      "input_proj_z2.bias: Yes\n",
      "input_proj_s2.weight: Yes\n",
      "input_proj_s2.bias: Yes\n",
      "norm_x1.weight: Yes\n",
      "norm_x1.bias: Yes\n",
      "norm_z1.weight: Yes\n",
      "norm_z1.bias: Yes\n",
      "norm_s1.weight: Yes\n",
      "norm_s1.bias: Yes\n",
      "norm_x2.weight: Yes\n",
      "norm_x2.bias: Yes\n",
      "norm_z2.weight: Yes\n",
      "norm_z2.bias: Yes\n",
      "norm_s2.weight: Yes\n",
      "norm_s2.bias: Yes\n",
      "W_x1.weight: Yes\n",
      "W_x1.bias: Yes\n",
      "W_z1.weight: Yes\n",
      "W_z1.bias: Yes\n",
      "W_s1.weight: Yes\n",
      "W_s1.bias: Yes\n",
      "W_x2.weight: Yes\n",
      "W_x2.bias: Yes\n",
      "W_z2.weight: Yes\n",
      "W_z2.bias: Yes\n",
      "W_s2.weight: Yes\n",
      "W_s2.bias: Yes\n",
      "norm_wx.weight: Yes\n",
      "norm_wx.bias: Yes\n",
      "norm_wz.weight: Yes\n",
      "norm_wz.bias: Yes\n",
      "norm_ws.weight: Yes\n",
      "norm_ws.bias: Yes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/Kioedru/code/SSGO\")\n",
    "ppi_feature_pre_model = torch.load(\n",
    "    \"/home/Kioedru/code/SSGO/codespace/pretrain/bimamba_seq1024/9606/bimamba_seq1024.pkl\",\n",
    "    map_location=\"cuda:0\",\n",
    ")\n",
    "print(ppi_feature_pre_model.transformerEncoder.encoder.layers)\n",
    "# 假设 finetune_model 是已经加载了预训练权重的模型\n",
    "\n",
    "# 冻结原模型（TransformerEncoder）的参数\n",
    "for layer in ppi_feature_pre_model.transformerEncoder.encoder.layers:\n",
    "    for param in layer.cross_attn.parameters():\n",
    "        param.requires_grad = True\n",
    "# for param in ppi_feature_pre_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "#     # print(param)\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # 只训练新添加的参数\n",
    "# # 例如 cross_layer1 和其他可能的新参数\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=1e-4\n",
    "# )\n",
    "\n",
    "# 检查哪些参数会被训练\n",
    "for name, param in ppi_feature_pre_model.named_parameters():\n",
    "    print(f\"{name}: {'Yes' if param.requires_grad else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ppi_feature_pre_model.state_dict(), \"ppi_feature_pre_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
