{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(32, 2)\n",
    "b = torch.rand(3, 32, 512)\n",
    "print(b[0].shape)\n",
    "print(a[:, 0:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 512])\n",
      "torch.Size([2, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "y = torch.stack([b[0], b[1]], dim=0)\n",
    "print(y.shape)\n",
    "y1 = b[0:2]\n",
    "print(y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a[:, 0:1] * b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "tensor([[0.1449, 0.1775, 0.1642,  ..., 0.0319, 0.2466, 0.0178],\n",
      "        [0.6853, 0.3168, 0.2989,  ..., 0.2124, 0.4165, 0.6602],\n",
      "        [0.4266, 0.5690, 0.2777,  ..., 0.6603, 0.6427, 0.6338],\n",
      "        ...,\n",
      "        [0.0490, 0.1528, 0.0098,  ..., 0.1614, 0.0325, 0.1531],\n",
      "        [0.0017, 0.2358, 0.2442,  ..., 0.2070, 0.0960, 0.1928],\n",
      "        [0.4192, 0.2027, 0.0420,  ..., 0.1097, 0.2987, 0.2615]])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GatedMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, hidden_dim, output_dim, num_experts, top_k=1, tau=1.0\n",
    "    ):\n",
    "        super(GatedMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList(\n",
    "            [FFN(input_dim, hidden_dim) for _ in range(num_experts)]\n",
    "        )\n",
    "        self.Wg = nn.Parameter(torch.randn(input_dim, num_experts))\n",
    "        self.b = nn.Parameter(torch.randn(num_experts))\n",
    "        # self.Wnoise = nn.Parameter(torch.randn(input_dim, num_experts))\n",
    "        self.top_k = top_k\n",
    "        # self.tau = tau\n",
    "        self.final_fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        batchsize = x_list[0].size(0)\n",
    "\n",
    "        expert_outputs = []\n",
    "        gates = []\n",
    "        expert_probabilities = []\n",
    "        for x in x_list:\n",
    "            # Step 1: Add noise\n",
    "            H = x @ self.Wg + self.b\n",
    "            # noise = torch.randn_like(H)  # StandardNormal()\n",
    "            # H += noise * F.softplus(x @ self.Wnoise)\n",
    "            expert_probabilities.append(F.softmax(H, dim=-1))\n",
    "            # Step 2: Keep top K values\n",
    "            top_k_values, _ = torch.topk(H, self.top_k, dim=-1)\n",
    "            mask = H >= top_k_values[:, -1, None]\n",
    "            H_top_k = H.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "            # Step 3: Apply Softmax\n",
    "            # G = F.gumbel_softmax(H_top_k, tau=self.tau, hard=False)\n",
    "            G = F.softmax(H_top_k, dim=1)\n",
    "            gates.append(G)\n",
    "            expert_outputs.append(\n",
    "                torch.stack([expert(x) for expert in self.experts], dim=1)\n",
    "            )\n",
    "\n",
    "        # Combine expert outputs with gates\n",
    "        combined_output = sum(\n",
    "            torch.sum(G.unsqueeze(-1) * e, dim=1) for G, e in zip(gates, expert_outputs)\n",
    "        ) / len(x_list)\n",
    "\n",
    "        output = self.final_fc(combined_output)\n",
    "\n",
    "        # expert_probabilities = [F.softmax(prob, dim=-1) for prob in expert_probabilities]\n",
    "        return output, expert_probabilities\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"计算给定分布 p 的熵\"\"\"\n",
    "    return -torch.sum(p * torch.log(p + 1e-9), dim=-1)\n",
    "\n",
    "\n",
    "def entropy_regularization_loss(expert_probabilities):\n",
    "    \"\"\"\n",
    "    计算熵正则化损失。\n",
    "\n",
    "    参数：\n",
    "    - expert_probabilities: 列表，包含每个模态的专家选择概率张量，形状为 [batch_size, num_experts]\n",
    "\n",
    "    返回：\n",
    "    - 熵正则化损失标量\n",
    "    \"\"\"\n",
    "    M = len(expert_probabilities)\n",
    "    batch_size = expert_probabilities[0].size(0)\n",
    "\n",
    "    # 计算每个模态的专家选择分布的熵\n",
    "    H_mj = [entropy(prob).mean() for prob in expert_probabilities]\n",
    "\n",
    "    # 计算整体专家选择分布\n",
    "    avg_prob = torch.stack(expert_probabilities, dim=0).mean(dim=0)\n",
    "    H_avg = entropy(avg_prob).mean()\n",
    "\n",
    "    # 计算熵正则化损失\n",
    "    E = torch.abs((1 / M) * sum(H_mj) - H_avg)\n",
    "\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 gate_hs 的形状为 [8, 32, 512]\n",
    "# weight 的形状为 [32, 8]\n",
    "import torch\n",
    "\n",
    "weight = torch.rand(32, 3)\n",
    "gate_hs = torch.rand(3, 32, 512)\n",
    "gate_hs_permuted = torch.einsum(\"LBD->BLD\", gate_hs)  # [32, 8, 512]\n",
    "\n",
    "weight_expanded = weight.unsqueeze(-1)  # [32, 8, 1]\n",
    "\n",
    "weighted_gate_hs = gate_hs_permuted * weight_expanded  # [32, 8, 512]\n",
    "\n",
    "weighted_gate_hs_final = torch.einsum(\"BLD->LBD\", weighted_gate_hs)  # [8, 32, 512]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
